{"name":"Cleansweep","tagline":"Ruby Utility for purging data in mysql","body":"Cleansweep\r\n\r\nUtilities for doing purges in an efficient, low-impact manner on \r\nmysql innodb tables.  Loosely based on the Percona pt-archive utility.\r\n\r\n## Installation\r\n\r\nAdd this line to your application's Gemfile:\r\n\r\n```ruby\r\ngem 'cleansweep'\r\n```\r\n\r\nAnd then execute:\r\n\r\n    $ bundle\r\n\r\nOr install it yourself as:\r\n\r\n    $ gem install cleansweep\r\n\r\n## How it works\r\n\r\nConsider the table:\r\n```sql\r\n    create table comments (\r\n       `id` int(11) primary key auto_increment,\r\n       `timestamp` datetime,\r\n       `account` int(11),\r\n       `liked` boolean,\r\n       key comments_on_account_timestamp(account, timestamp)\r\n    )\r\n```\r\nAssume there is an active record model for it:\r\n\r\n    class Comment < ActiveRecord::Base ; end\r\n\r\n### Purging by traversing an index\r\n\r\nThe most efficient way to work through a table is by scanning through an index one chunk\r\nat a time.\r\n\r\nLet's assume we want to purge Comments older than 1 month.  We can\r\nscan the primary key index or the `account`,`timestamp` index.  In this case the latter will\r\nprobably work better since we are evaluating the timestamp for the purge.\r\n\r\n```ruby\r\n    r = CleanSweep::PurgeRunner.new model: Comment,\r\n                                    index: 'comments_on_account_timestamp' do | scope |\r\n        scope.where('timestamp < ?', 1.month.ago)\r\n    end\r\n```\r\n\r\nTo execute the purge, do:\r\n\r\n```ruby\r\n    count = r.execute_in_batches\r\n    puts \"Deleted #{count} rows\"\r\n```\r\n\r\nCheck what it will do:\r\n\r\n```ruby\r\n    r.print_queries($stdout)\r\n```\r\n\r\nThis will show you what it will do by printing out the three different statements used:\r\n\r\n```sql\r\n    Initial Query:\r\n        SELECT  `id`,`account`,`timestamp`\r\n        FROM `comments` FORCE INDEX(comments_on_account_timestamp)\r\n        WHERE (timestamp < '2014-11-25 21:47:43')\r\n        ORDER BY `account` ASC,`timestamp` ASC\r\n        LIMIT 500\r\n    Chunk Query:\r\n        SELECT  `id`,`account`,`timestamp`\r\n        FROM `comments` FORCE INDEX(comments_on_account_timestamp)\r\n        WHERE (timestamp < '2014-11-25 21:47:43') AND (`account` > 0 OR (`account` = 0 AND `timestamp` > '2014-11-18 21:47:43'))\\n    ORDER BY `account` ASC,`timestamp` ASC\r\n        LIMIT 500\r\n    Delete Statement:\r\n        DELETE\r\n        FROM `comments`\r\n        WHERE (`id` = 2)\r\n```\r\n\r\nIt does the initial statement once to get the first chunk of rows.  Then it does subsequent queries\r\nstarting at the index where the last chunk left off, thereby avoiding a complete index scan.  This works\r\nfine as long as you don't have rows with duplicate account id and timestamps.  If you do, you'll possibly\r\nmiss rows between chunks.\r\n\r\nTo avoid missing duplicates, you can traverse the index using only the first column with an inclusive comparator\r\nlike `>=` instead of `>`.  Here's what that would look like:\r\n\r\n```ruby\r\n    r = CleanSweep::PurgeRunner.new model:Comment,\r\n                                    index: 'comments_on_account_timestamp',\r\n                                    first_only: true do | scope |\r\n        scope.where('timestamp < ?', 1.month.ago)\r\n    end\r\n```\r\n\r\nThe chunk query looks like:\r\n\r\n```sql\r\n    SELECT  `id`,`account`,`timestamp`\r\n    FROM `comments` FORCE INDEX(comments_on_account_timestamp)\r\n    WHERE (timestamp < '2014-11-25 21:47:43') AND (`account` >= 0)\r\n    LIMIT 500\r\n```\r\n\r\nYou can scan the index in either direction.  To specify descending order, use the `reverse: true` option.\r\n\r\n### Copying rows from one table to another\r\n\r\nYou can use the same technique to copy rows from one table to another.  Support in CleanSweep is pretty\r\nminimal.  It won't _move_ rows, only copy them, although it would be easy to fix this.\r\nI used this to copy ids into a temporary table which I then\r\nused to delete later.\r\n\r\nHere's an example that copies rows from the `Comment` model to the `ExpiredComment` model (`expired_comments`).\r\nComments older than one week are copied.\r\n\r\n```ruby\r\n      copier = CleanSweep::PurgeRunner.new model: Comment,\r\n                                           index: 'comments_on_account_timestamp',\r\n                                           dest_model: ExpiredComment,\r\n                                           copy_columns: %w[liked] do do | model |\r\n        model.where('last_used_at < ?', 1.week.ago)\r\n      end\r\n```\r\n\r\nThe `copy_columns` option specifies additional columns to be inserted into the `expired_comments` table.\r\n\r\n### Watching the history list and replication lag\r\n\r\nYou can enter thresholds for the history list size and replication lag that will be used to pause the\r\npurge if either of those values get into an unsafe territory.  The script will pause for 5 minutes and\r\nonly start once the corresponding metric goes back down to 90% of the specified threshold.\r\n\r\n### Logging and monitoring progress\r\n\r\nYou pass in a standard log instance to capture all running output.  By default it will log to your\r\n`ActiveRecord::Base` logger, or stdout if that's not set up.\r\n\r\nIf you specify a reporting interval\r\nwith the `report` option it will print the status of the purge at that interval.  This is useful to track\r\nprogress and assess the rate of deletion.\r\n\r\n### Joins and subqueries\r\n\r\nYou can add subqueries and joins to your query in the scope block, but be careful.  The index and order\r\nclause may work against you if the table you are joining with doesn't have good parity with the indexes\r\nin your target table.\r\n\r\n### Limitations\r\n\r\n* Only works for mysql (as far as I know).  I have only used it against 5.5.\r\n* Should work with ActiveRecord 3.* - 4.*.\r\n* Using a non-unique index risks missing duplicate rows unless you use the `first_only` option.\r\n* Using the `first_only` option risks rescanning many rows if you have many more duplicates than your\r\n  chunk size\r\n* An index is required but you should be able to run a purge without one.  It just means you're not\r\n  scanning the index in chunks.  This might be okay if you are deleting everything as you go along because\r\n  then you're not rescanning the rows.  It wouldn't require much to modify CleanSweep to support this\r\n  mode.\r\n\r\n### Other options\r\n\r\nThere are a number of other options you can use to tune the script.  For details look at the\r\n[API on the `PurgeRunner` class](http://bkayser.github.io/cleansweep/rdoc/CleanSweep/PurgeRunner.html)\r\n\r\n### NewRelic integration\r\n\r\nThe script requires the [New Relic](http://github.com/newrelic/rpm) gem.  It won't impact anyting if you\r\ndon't have a New Relic account to report to, but if you do use New Relic it is configured to show you\r\ndetailed metrics.  I recommend turning off transaction traces for long purge jobs to reduce your memory\r\nfootprint.\r\n\r\n## Testing\r\n\r\nTo run the specs, start a local mysql instance.  The default user is root with an empty password.\r\nOverride the user/password with environment variables `DB_USER` and `DB_PASSWORD`.  The test\r\ncreates a db called 'cstest'.\r\n\r\n## Contributing\r\n\r\n1. Fork it ( https://github.com/bkayser/cleansweep/fork )\r\n2. Create your feature branch (`git checkout -b my-new-feature`)\r\n3. Commit your changes (`git commit -am 'Add some feature'`)\r\n4. Push to the branch (`git push origin my-new-feature`)\r\n5. Create a new Pull Request\r\n\r\n## License and Copyright\r\n\r\nCopyright 2014 New Relic, Inc., and Bill Kayser\r\n\r\nCovered by the MIT [LICENSE](LICENSE.txt).\r\n\r\n### Credits\r\n\r\nThis was all inspired and informed by [Percona's `pt-archiver` script](http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html)\r\nwritten by Baron Schwartz.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}